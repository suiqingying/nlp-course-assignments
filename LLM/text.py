import time
import json
import random
import os
import re
from anthropic import Anthropic
from sklearn.metrics import accuracy_score

# ================= é…ç½®åŒºåŸŸ =================
os.environ["ANTHROPIC_BASE_URL"] = "http://www.claudecodeserver.top/api"
os.environ["ANTHROPIC_API_KEY"] = "sk_317d87cb3cf64fde228486c6d3d397b181eee1c7b42865a3ae5f9e1395f991d3"

API_KEY = "sk_317d87cb3cf64fde228486c6d3d397b181eee1c7b42865a3ae5f9e1395f991d3"
BASE_URL = "http://www.claudecodeserver.top/api"
MODEL_NAME = "claude-sonnet-4-5-20250929"

# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))

# è¿™é‡Œå¡«å†™ä½ çš„æ–‡ä»¶å
TRAIN_FILE = os.path.join(SCRIPT_DIR, "train.jsonl")  # ä½ çš„è®­ç»ƒé›†æ–‡ä»¶å
TEST_FILE = os.path.join(SCRIPT_DIR, "test.jsonl")    # ä½ çš„æµ‹è¯•é›†æ–‡ä»¶å

BATCH_SIZE = 100     # æ¯æ¬¡ API è°ƒç”¨å¤„ç†çš„æ¡æ•°
TEST_LIMIT = 200     # æ€»å…±æµ‹è¯•æ¡æ•°
FEW_SHOT_NUM = 10   # Few-shot ç¤ºä¾‹æ•°é‡
# ===========================================

client = Anthropic(
    api_key=API_KEY, 
    base_url=BASE_URL,
    default_headers={
        "anthropic-version": "2023-06-01",
    }
)

def process_line(json_line):
    """
    å¤„ç†å•è¡Œæ•°æ®ï¼š
    1. æ‹¼æ¥åˆ†è¯åˆ—è¡¨ -> å­—ç¬¦ä¸²
    2. æ˜ å°„æ•°å­—æ ‡ç­¾ -> ä¸­æ–‡
    """
    item = json.loads(json_line.strip())
    
    # 1. æ‹¼æ¥ï¼š["æˆ¿é—´", "è¿˜", "å¯ä»¥"] -> "æˆ¿é—´è¿˜å¯ä»¥"
    # è¿™ä¸€æ­¥å¾ˆé‡è¦ï¼å¤§æ¨¡å‹è¯»åˆ—è¡¨æ•ˆæœä¸å¥½ï¼Œè¯»æ•´å¥æ•ˆæœæ‰å¥½ã€‚
    text_content = "".join(item["text"])
    
    # 2. æ˜ å°„ï¼š0 -> è´Ÿå‘, 1 -> æ­£å‘
    # è¯·æ ¹æ®ä½ çš„æ•°æ®å®é™…æƒ…å†µä¿®æ”¹ï¼Œé€šå¸¸ 0æ˜¯è´Ÿå‘ï¼Œ1æ˜¯æ­£å‘
    label_map = {"0": "è´Ÿå‘", "1": "æ­£å‘"}
    
    # æ³¨æ„ï¼šä½ çš„jsoné‡Œlabelæ˜¯å­—ç¬¦ä¸²"0"è¿˜æ˜¯æ•°å­—0ï¼Ÿè¿™é‡Œåšäº†å…¼å®¹å¤„ç†
    raw_label = str(item["label"]) 
    human_label = label_map.get(raw_label, "æœªçŸ¥")
    
    return {"text": text_content, "label": human_label}

def load_dataset(file_path, is_test_file=False):
    data_list = []
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    # æ‰“ä¹±é¡ºåºï¼Œæ¶ˆé™¤ä½ç½®åå·®
    if is_test_file:
        random.shuffle(lines)
        if len(lines) > TEST_LIMIT:
            lines = lines[:TEST_LIMIT]
        
    for line in lines:
        if not line.strip(): continue
        data_list.append(process_line(line))
        
    return data_list

def balance_sample(data_list, num_samples):
    """
    ä»æ•°æ®ä¸­å‡è¡¡é‡‡æ ·æŒ‡å®šæ•°é‡çš„æ ·æœ¬
    ç¡®ä¿æ­£å‘å’Œè´Ÿå‘æ ‡ç­¾çš„æ•°é‡ç›¸ç­‰æˆ–æ¥è¿‘
    """
    positive = [d for d in data_list if d['label'] == 'æ­£å‘']
    negative = [d for d in data_list if d['label'] == 'è´Ÿå‘']
    
    # æ¯ä¸ªç±»åˆ«é‡‡æ ·çš„æ•°é‡
    per_class = num_samples // 2
    
    # éšæœºé‡‡æ ·
    sampled_positive = random.sample(positive, min(per_class, len(positive)))
    sampled_negative = random.sample(negative, min(per_class, len(negative)))
    
    # åˆå¹¶å¹¶æ‰“ä¹±
    balanced_samples = sampled_positive + sampled_negative
    random.shuffle(balanced_samples)
    
    return balanced_samples

def get_batch_prediction(batch_data, few_shot_examples=None):
    """
    æ ¸å¿ƒäº¤äº’å‡½æ•°
    """
    # ================= æ„é€  Prompt (é˜²æ­¢æ³„éœ²çš„å…³é”®åœ¨è¿™é‡Œ) =================
    prompt = "ä»»åŠ¡ï¼šæƒ…æ„Ÿåˆ†ç±»ã€‚è¯·åˆ¤æ–­ä»¥ä¸‹è¯„è®ºæ˜¯ã€æ­£å‘ã€‘è¿˜æ˜¯ã€è´Ÿå‘ã€‘ã€‚\n\n"
    
    # Part 1: ä¸Šä¸‹æ–‡ç¤ºä¾‹ (æ¥è‡ªè®­ç»ƒé›†)
    # è¿™é‡Œå¿…é¡»æŠŠ Label æ”¾è¿›å»ï¼Œå› ä¸ºè¿™æ˜¯æ•™æ¨¡å‹æ€ä¹ˆåš
    if few_shot_examples:
        prompt += "=== å‚è€ƒæ ·ä¾‹ (ä»…ä¾›å­¦ä¹ ) ===\n"
        for ex in few_shot_examples:
            prompt += f"è¯„è®º: {ex['text']}\næƒ…æ„Ÿ: {ex['label']}\n---\n"
        prompt += "\n"

    # Part 2: å¾…æµ‹æ•°æ® (æ¥è‡ªæµ‹è¯•é›†)
    # ã€ç»å¯¹é‡ç‚¹ã€‘ï¼šè¿™é‡Œåªæ”¾ textï¼Œä¸æ”¾ labelï¼
    prompt += "=== è¯·å¯¹ä»¥ä¸‹è¯„è®ºè¿›è¡Œåˆ†ç±» ===\n"
    for idx, item in enumerate(batch_data):
        # è¿™é‡Œçš„ item['text'] å°±æ˜¯ "æˆ¿é—´è¿˜å¯ä»¥..."
        # æˆ‘ä»¬æ²¡æœ‰æŠŠ item['label'] æ”¾è¿›å»ï¼Œè¿™å°±æ˜¯é˜²æ­¢æ³„éœ²
        prompt += f"{idx+1}. {item['text']}\n"
    
    prompt += f"\nè¯·ç›´æ¥è¾“å‡º {len(batch_data)} è¡Œç»“æœï¼Œæ¯è¡Œä¸€ä¸ªæ ‡ç­¾ï¼ˆæ­£å‘/è´Ÿå‘ï¼‰ã€‚ä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ã€‚"
    # =================================================================

    try:
        message = client.messages.create(
            model=MODEL_NAME,
            max_tokens=2000,
            temperature=0.0,
            system='ä½ æ˜¯ä¸€ä¸ªæƒ…æ„Ÿåˆ†æåŠ©æ‰‹ã€‚åªè¾“å‡ºæ ‡ç­¾ã€‚',
            messages=[
                {'role': 'user', 'content': prompt}
            ]
        )
        content = message.content[0].text.strip()
        # ç®€å•æ¸…æ´—ç»“æœï¼šå»æ‰ç¼–å·ï¼ˆå¦‚ "1. æ­£å‘" -> "æ­£å‘"ï¼‰
        lines = []
        for line in content.split('\n'):
            line = line.strip()
            if not line: continue
            # å»æ‰å¼€å¤´çš„ç¼–å·ï¼Œå¦‚ "1. ", "12. ", "123."
            line = re.sub(r'^\d+[\.\ã€\)\]\s]+\s*', '', line)
            lines.append(line)
        
        # è¿”å›é¢„æµ‹ç»“æœå’Œ token ä¿¡æ¯
        tokens_info = {
            'prompt_tokens': message.usage.input_tokens,
            'completion_tokens': message.usage.output_tokens,
            'total_tokens': message.usage.input_tokens + message.usage.output_tokens
        }
        return lines[:len(batch_data)], tokens_info
    except Exception as e:
        print(f"Error: {e}")
        return ["Error"] * len(batch_data), None

def log(message, file_handle):
    """åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶"""
    print(message)
    file_handle.write(message + "\n")

def main():
    # æ‰“å¼€æ—¥å¿—æ–‡ä»¶
    output_file = os.path.join(SCRIPT_DIR, "results.txt")
    f = open(output_file, 'w', encoding='utf-8')
    
    # è¾“å‡ºæµ‹è¯•é…ç½®ä¿¡æ¯
    log("="*50, f)
    log("ğŸ“ æµ‹è¯•é…ç½®ä¿¡æ¯", f)
    log("="*50, f)
    log(f"æ¨¡å‹: {MODEL_NAME}", f)
    log(f"API: {BASE_URL}", f)
    log(f"æµ‹è¯•æ•°æ®é™åˆ¶: {TEST_LIMIT} æ¡", f)
    log(f"Few-shot ç¤ºä¾‹æ•°: {FEW_SHOT_NUM} æ¡", f)
    log(f"Batch å¤§å°: {BATCH_SIZE} æ¡", f)
    log(f"è®­ç»ƒé›†æ–‡ä»¶: {TRAIN_FILE}", f)
    log(f"æµ‹è¯•é›†æ–‡ä»¶: {TEST_FILE}", f)
    log(f"æµ‹è¯•æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}", f)
    log("="*50 + "\n", f)
    
    log("1. æ­£åœ¨åŠ è½½æ•°æ®...", f)
    # åŠ è½½è®­ç»ƒé›†å…¨é‡
    full_train = load_dataset(TRAIN_FILE, is_test_file=False)
    # ä»è®­ç»ƒé›†é‡Œå‡è¡¡é‡‡æ · Few-shot ä¾‹å­
    few_shot_examples = balance_sample(full_train, FEW_SHOT_NUM)
    
    # åŠ è½½æµ‹è¯•é›† (é‡‡æ ·å)
    test_data = load_dataset(TEST_FILE, is_test_file=True)
    
    log(f"   è®­ç»ƒé›†æ ·ä¾‹æ•°é‡: {len(few_shot_examples)}", f)
    positive_count = sum(1 for ex in few_shot_examples if ex['label'] == 'æ­£å‘')
    negative_count = sum(1 for ex in few_shot_examples if ex['label'] == 'è´Ÿå‘')
    log(f"   æ­£å‘: {positive_count} | è´Ÿå‘: {negative_count}", f)
    for idx, ex in enumerate(few_shot_examples):
        log(f"   ç¤ºä¾‹ {idx+1}: {ex}", f)
    log(f"   å‡†å¤‡æµ‹è¯• {len(test_data)} æ¡æ•°æ®...", f)

    # ================= å¼€å§‹å®éªŒ (Few-shot) =================
    log("\nğŸš€ å¼€å§‹è¿è¡Œ Few-shot å®éªŒ...", f)
    all_preds = []
    all_truths = [item['label'] for item in test_data] # çœŸå®æ ‡ç­¾å­˜åœ¨è¿™ï¼Œä¸å‘ç»™ API
    total_tokens = 0
    
    # æ‰¹å¤„ç†å¾ªç¯
    for i in range(0, len(test_data), BATCH_SIZE):
        batch = test_data[i : i + BATCH_SIZE]
        log(f"Processing batch {i//BATCH_SIZE + 1}/{(len(test_data)-1)//BATCH_SIZE + 1} ({len(batch)} æ¡)...", f)
        
        # è°ƒç”¨ API
        preds, tokens_info = get_batch_prediction(batch, few_shot_examples)
        
        # è¡¥é½é•¿åº¦ï¼ˆä¸‡ä¸€æ¨¡å‹åªå›äº†éƒ¨åˆ†ï¼Œé˜²æ­¢æŠ¥é”™ï¼‰
        while len(preds) < len(batch): preds.append("Error")
        
        # ç´¯è®¡ token æ•°
        if tokens_info:
            total_tokens += tokens_info['total_tokens']
            log(f"Done. (Token: {tokens_info['total_tokens']})", f)
        else:
            log("Done.", f)
            
        all_preds.extend(preds)
        time.sleep(1) # ä¼‘æ¯ä¸€ä¸‹

    # ================= è®¡ç®—å‡†ç¡®ç‡ =================
    # ç®€å•æ¸…æ´—æ•°æ®ï¼ˆé˜²æ­¢ label ä¸ä¸€è‡´ï¼‰
    clean_preds = []
    clean_truths = []
    for p, t in zip(all_preds, all_truths):
        # åªè¦åŒ…å«äº†å…³é”®è¯å°±ç®—å¯¹ (Claudeæœ‰æ—¶å€™ä¼šå› "æ˜¯æ­£å‘")
        p_clean = "æ­£å‘" if "æ­£å‘" in p else ("è´Ÿå‘" if "è´Ÿå‘" in p else "Error")
        if p_clean != "Error":
            clean_preds.append(p_clean)
            clean_truths.append(t)
    
    acc = accuracy_score(clean_truths, clean_preds)
    
    # è¾“å‡ºç»“æœ
    log("\n" + "="*50, f)
    log("ğŸ“‹ æµ‹è¯•é›†ç»“æœè¯¦æƒ…:", f)
    log("="*50, f)
    for idx, (text, true_label, pred_label) in enumerate(zip([item['text'] for item in test_data], all_truths, all_preds)):
        match = "âœ…" if ("æ­£å‘" in pred_label if true_label == "æ­£å‘" else "è´Ÿå‘" in pred_label) else "âŒ"
        log(f"{idx+1}. {match} æ–‡æœ¬: {text}", f)
        log(f"   çœŸå®: {true_label} | é¢„æµ‹: {pred_label}\n", f)
            
    log("="*50, f)
    log(f"ğŸ“Š æœ€ç»ˆå‡†ç¡®ç‡: {acc:.2%}", f)
    log(f"ğŸ’° æ€» Token æ¶ˆè€—: {total_tokens}", f)
    log("="*50, f)
    
    f.close()
    print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

if __name__ == "__main__":
    main()