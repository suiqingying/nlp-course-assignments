import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from collections import namedtuple
import logging
import time
import json
import os
from common import SentimentDataset, collate_fn, evaluate

class ChannelAttention(nn.Module):
    """é€šé“æ³¨æ„åŠ›æœºåˆ¶"""
    def __init__(self, in_channels, reduction=16):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool1d(1)
        self.max_pool = nn.AdaptiveMaxPool1d(1)
        self.fc = nn.Sequential(
            nn.Linear(in_channels, in_channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(in_channels // reduction, in_channels, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        # x shape: [batch, channels, length]
        b, c, _ = x.size()

        # å¹³å‡æ± åŒ–åˆ†æ”¯
        avg_out = self.fc(self.avg_pool(x).view(b, c))

        # æœ€å¤§æ± åŒ–åˆ†æ”¯
        max_out = self.fc(self.max_pool(x).view(b, c))

        # èåˆ
        out = avg_out + max_out
        out = out.view(b, c, 1)
        return x * out.expand_as(x)

class SpatialAttention(nn.Module):
    """ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶"""
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()
        self.conv = nn.Conv1d(2, 1, kernel_size, padding=kernel_size//2, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # x shape: [batch, channels, length]
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        combined = torch.cat([avg_out, max_out], dim=1)
        attention = self.sigmoid(self.conv(combined))
        return x * attention

class DualAttentionBlock(nn.Module):
    """åŒé‡æ³¨æ„åŠ›å—"""
    def __init__(self, channels):
        super(DualAttentionBlock, self).__init__()
        self.channel_att = ChannelAttention(channels)
        self.spatial_att = SpatialAttention()
        self.norm1 = nn.LayerNorm(channels)
        self.norm2 = nn.LayerNorm(channels)

    def forward(self, x):
        # x shape: [batch, channels, length]
        residual = x

        # é€šé“æ³¨æ„åŠ›
        x = self.channel_att(x)
        x = self.norm1(x.transpose(1, 2)).transpose(1, 2) + residual

        # ç©ºé—´æ³¨æ„åŠ›
        residual = x
        x = self.spatial_att(x)
        x = self.norm2(x.transpose(1, 2)).transpose(1, 2) + residual

        return x

class UltraTextCNN(nn.Module):
    """
    æé™ç‰ˆTextCNN - æœ€å…ˆè¿›æ¶æ„ç»“åˆ
    """
    def __init__(self, config):
        super(UltraTextCNN, self).__init__()

        self.vocab_size = config.vocab_size
        self.embedding_dim = config.embedding_dim if hasattr(config, 'embedding_dim') else 400
        self.num_classes = config.num_classes
        self.num_heads = config.num_heads if hasattr(config, 'num_heads') else 8

        # 1. è¯åµŒå…¥å±‚ - é«˜ç»´åº¦
        self.embedding = nn.Embedding(self.vocab_size + 1, self.embedding_dim, padding_idx=8019)

        # 2. ä½ç½®ç¼–ç 
        self.pos_embedding = nn.Embedding(2048, self.embedding_dim)

        # 3. æ®µå·®ç¼–ç 
        self.segment_embedding = nn.Embedding(4, self.embedding_dim)

        # 4. åˆ†å±‚å·ç§¯æ¶æ„
        self.conv_layers = nn.ModuleDict()
        filter_sizes = [2, 3, 4, 5, 7]  # æ›´å¤šçš„å°ºåº¦
        channels_list = [128, 256, 384, 512, 256]

        for i, (fs, ch) in enumerate(zip(filter_sizes, channels_list)):
            # æ¯å±‚åŒ…å«å¤šçº§å·ç§¯
            layer_name = f'conv_{i}'
            self.conv_layers[layer_name] = nn.ModuleDict({
                'conv1': nn.Conv2d(1, ch, (fs, self.embedding_dim), padding=(fs//2, 0)),
                'conv2': nn.Conv2d(ch, ch*2, (3, 1), padding=(1, 0)),
                'conv3': nn.Conv2d(ch*2, ch*2, (3, 1), padding=(1, 0)),
                'bn1': nn.BatchNorm2d(ch),
                'bn2': nn.BatchNorm2d(ch*2),
                'bn3': nn.BatchNorm2d(ch*2),
                'attention': DualAttentionBlock(ch*2),
                'dropout': nn.Dropout(0.2 + i*0.05)
            })

        # 5. å¯†é›†è¿æ¥æ¨¡å—
        total_channels = sum(channels_list)*2  # æ¯å±‚è¾“å‡ºé€šé“æ•°ç¿»å€
        self.dense_layers = nn.ModuleList([
            nn.Linear(total_channels, total_channels // 2),
            nn.BatchNorm1d(total_channels // 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(total_channels // 2, total_channels // 4),
            nn.BatchNorm1d(total_channels // 4),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(total_channels // 4, total_channels // 8),
            nn.BatchNorm1d(total_channels // 8),
            nn.ReLU(),
            nn.Dropout(0.2)
        ])

        # 6. Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=total_channels // 8,
            nhead=self.num_heads,
            dim_feedforward=1024,
            dropout=0.1,
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3)

        # 7. è‡ªæ³¨æ„åŠ›æ± åŒ–
        self.attention_pooling = nn.Sequential(
            nn.Linear(total_channels // 8, total_channels // 16),
            nn.Tanh(),
            nn.Linear(total_channels // 16, 1)
        )

        # 8. å¤šå±‚åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(total_channels // 8, 512),
            nn.LayerNorm(512),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(512, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, self.num_classes)
        )

        # 9. è¾…åŠ©æŸå¤±å¤´
        self.aux_classifier = nn.Sequential(
            nn.Linear(total_channels // 8, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 2)
        )

        # æƒé‡åˆå§‹åŒ–
        self._init_weights()

    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Embedding):
                nn.init.normal_(m.weight, 0, 0.02)

    def forward(self, x):
        batch_size, seq_len = x.size()

        # 1. è¯åµŒå…¥ + ä½ç½®ç¼–ç 
        embedded = self.embedding(x)

        # ä½ç½®ç¼–ç 
        if seq_len <= 2048:
            pos = torch.arange(0, seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)
            embedded = embedded + self.pos_embedding(pos)

        # æ®µå·®ç¼–ç ï¼ˆæ¨¡æ‹Ÿå¥å­çš„ä¸åŒéƒ¨åˆ†ï¼‰
        segment_ids = torch.zeros_like(x)
        segment_ids[:, seq_len//2:] = 1
        if seq_len // 3 < seq_len:
            segment_ids[:, seq_len//3:] = 2
        if seq_len // 4 < seq_len:
            segment_ids[:, seq_len//4:] = 3
        embedded = embedded + self.segment_embedding(segment_ids)

        # 2. åˆ†å±‚å·ç§¯ç‰¹å¾æå–
        conv_outputs = []
        for layer_name in self.conv_layers:
            layer_dict = self.conv_layers[layer_name]
            # ç¬¬ä¸€å±‚å·ç§¯
            x_conv = embedded.unsqueeze(1)
            x_conv = layer_dict.conv1(x_conv)
            x_conv = layer_dict.bn1(x_conv)
            x_conv = F.relu(x_conv)
            x_conv = layer_dict.dropout(x_conv)

            # ç¬¬äºŒå±‚å·ç§¯
            x_conv = layer_dict.conv2(x_conv)
            x_conv = layer_dict.bn2(x_conv)
            x_conv = F.relu(x_conv)
            x_conv = layer_dict.dropout(x_conv)

            # ç¬¬ä¸‰å±‚å·ç§¯
            x_conv = layer_dict.conv3(x_conv)
            x_conv = layer_dict.bn3(x_conv)
            x_conv = F.relu(x_conv)

            # é‡å¡‘ä»¥é€‚åº”æ³¨æ„åŠ›
            x_conv = x_conv.squeeze(-1)  # [batch, channels, seq_len]
            x_conv = layer_dict.attention(x_conv)

            # å…¨å±€æœ€å¤§æ± åŒ–
            pooled = F.max_pool1d(x_conv, x_conv.size(2))  # [batch, channels, 1]
            pooled = pooled.squeeze(-1)  # [batch, channels]
            conv_outputs.append(pooled)

        # 3. ç‰¹å¾èåˆ
        feature_map = torch.cat(conv_outputs, dim=1)  # [batch, total_channels]

        # 4. å¯†é›†è¿æ¥
        for dense_layer in self.dense_layers:
            feature_map = dense_layer(feature_map)

        # 5. Transformerå¤„ç†
        x_trans = feature_map.unsqueeze(1)  # [batch, 1, features]
        x_trans = self.transformer_encoder(x_trans)
        x_trans = x_trans.squeeze(1)  # [batch, features]

        # 6. è‡ªæ³¨æ„åŠ›æ± åŒ–
        attention_weights = self.attention_pooling(x_trans)
        attention_weights = F.softmax(attention_weights, dim=1)
        pooled_features = attention_weights * x_trans

        # 7. ä¸»åˆ†ç±»è¾“å‡º
        main_output = self.classifier(pooled_features)

        # 8. è¾…åŠ©è¾“å‡ºï¼ˆè®­ç»ƒæ—¶ä½¿ç”¨ï¼‰
        aux_output = self.aux_classifier(pooled_features)

        if self.training:
            return main_output, aux_output
        else:
            return main_output

    def predict(self, x):
        self.eval()
        with torch.no_grad():
            logits = self.forward(x)
            if isinstance(logits, tuple):
                logits = logits[0]
            return torch.argmax(logits, dim=-1)

def create_ultra_config(vocab_size):
    """åˆ›å»ºæé™é…ç½®"""
    config = {
        'vocab_size': vocab_size,
        'embedding_dim': 400,
        'num_classes': 2,
        'filter_sizes': [2, 3, 4, 5, 7],
        'num_heads': 8,
        'dropout': 0.3,
        'lr': 3e-4,
        'weight_decay': 1e-4,
        'eval_interval': 30,
        'num_epoch': 25,
        'save_path': '../save_model/ultra_textcnn_best.pt',
        'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),
        'log_steps': 30,
        'batch_size': 32
    }
    return config

def train_ultra_model():
    """è®­ç»ƒæé™ç‰ˆTextCNN"""
    print("=" * 60)
    print("å¼€å§‹è®­ç»ƒæé™ç‰ˆTextCNNæ¨¡å‹")
    print("=" * 60)

    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('ultra_training.log', mode='w', encoding='utf-8'),
            logging.StreamHandler()
        ]
    )

    print("\n1. åŠ è½½æ•°æ®é›†...")
    # åŠ è½½æ•°æ®
    train_dataset = SentimentDataset('../dataset/train.jsonl', '../dataset/vocab.json')
    val_dataset = SentimentDataset('../dataset/val.jsonl', '../dataset/vocab.json')
    test_dataset = SentimentDataset('../dataset/test.jsonl', '../dataset/vocab.json')

    print(f"   è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_dataset)}")
    print(f"   éªŒè¯é›†æ ·æœ¬æ•°: {len(val_dataset)}")
    print(f"   æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_dataset)}")

    print("\n2. åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    # DataLoader
    from torch.utils.data import DataLoader
    train_loader = DataLoader(
        train_dataset,
        batch_size=32,  # æ›´å°çš„batch sizeæ”¯æŒæ›´å¤§æ¨¡å‹
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=4,
        pin_memory=True
    )
    val_loader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn)
    test_loader = DataLoader(test_dataset, batch_size=1, collate_fn=collate_fn)
    print("   æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ")

    print("\n3. åˆ›å»ºæ¨¡å‹...")
    # åˆ›å»ºæ¨¡å‹é…ç½®
    vocab_size = len(json.load(open('../dataset/vocab.json', 'r', encoding='utf-8')))
    config_dict = create_ultra_config(vocab_size)
    config = namedtuple('config', config_dict.keys())(**config_dict)

    device = config.device
    model = UltraTextCNN(config).to(device)

    print(f"   è¯æ±‡è¡¨å¤§å°: {vocab_size}")
    print(f"   æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    print(f"   ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"   åµŒå…¥ç»´åº¦: {config.embedding_dim}")
    print(f"   å·ç§¯æ ¸å¤§å°: {config.filter_sizes}")
    print(f"   æ³¨æ„åŠ›å¤´æ•°: {config.num_heads}")
    print(f"   å­¦ä¹ ç‡: {config.lr}")
    print(f"   Batch Size: {config.batch_size}")

    # æ˜¾ç¤ºCUDAä¿¡æ¯
    if device.type == 'cuda':
        print(f"   GPU: {torch.cuda.get_device_name()}")
        print(f"   GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        print(f"   å½“å‰GPUä½¿ç”¨: {torch.cuda.memory_allocated()/1024**3:.2f}GB")

    print("\n4. å¼€å§‹è®­ç»ƒ...")
    print("-" * 60)
    logging.info("=== æé™ç‰ˆTextCNNè®­ç»ƒå¼€å§‹ ===")

    # è‡ªå®šä¹‰è®­ç»ƒå‡½æ•°ä»¥æ”¯æŒè¾…åŠ©æŸå¤±
    def train_ultra_custom(model, config, train_loader, val_loader):
        criterion = nn.CrossEntropyLoss()
        aux_criterion = nn.CrossEntropyLoss()

        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=config.lr,
            weight_decay=1e-5,
            betas=(0.9, 0.95)
        )

        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='max',
            factor=0.7,
            patience=3
        )

        logging.info("å¼€å§‹è®­ç»ƒ...")
        model.train()
        best_acc = 0.0
        train_loss_history = []
        val_acc_history = []
        step = 0

        for epoch in range(config.num_epoch):
            logging.info(f"Epoch {epoch + 1}/{config.num_epoch}")
            epoch_loss = 0
            model.train()

            for i, data in enumerate(train_loader):
                step += 1
                inputs = data[0].to(config.device)
                labels = data[1].to(config.device)

                optimizer.zero_grad()
                outputs, aux_outputs = model(inputs)

                # ä¸»æŸå¤± + è¾…åŠ©æŸå¤±
                main_loss = criterion(outputs, labels)
                aux_loss = aux_criterion(aux_outputs, labels)
                total_loss = main_loss + 0.3 * aux_loss

                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()

                epoch_loss += total_loss.item()

                if step % config.eval_interval == 0:
                    model.eval()
                    correct = 0
                    total = 0

                    with torch.no_grad():
                        for data in val_loader:
                            inputs = data[0].to(config.device)
                            labels = data[1].to(config.device)
                            outputs = model(inputs)
                            if isinstance(outputs, tuple):
                                outputs = outputs[0]

                            _, predicted = torch.max(outputs.data, 1)
                            total += labels.size(0)
                            correct += (predicted == labels).sum().item()

                    val_acc = correct / total
                    avg_loss = epoch_loss / (i + 1)

                    logging.info(f'Step {step}: Loss: {avg_loss:.4f}, Val Acc: {val_acc:.4f}')
                    val_acc_history.append(val_acc)

                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    if val_acc > best_acc:
                        best_acc = val_acc
                        logging.info(f'ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯é›†å‡†ç¡®ç‡: {best_acc:.4f}')
                        save_dict = {
                            'model_state_dict': model.state_dict(),
                            'config': config._asdict() if hasattr(config, '_asdict') else config.__dict__
                        }
                        torch.save(save_dict, config.save_path)

                    scheduler.step(val_acc)
                    model.train()

            train_loss_history.append(epoch_loss / len(train_loader))
            logging.info(f'Epoch {epoch + 1} å®Œæˆï¼Œå¹³å‡æŸå¤±: {epoch_loss / len(train_loader):.4f}')

        return train_loss_history, val_acc_history

    # è®­ç»ƒæ¨¡å‹
    print("\n   è®­ç»ƒè¿›åº¦:")
    start_time = time.time()
    train_loss_history, val_acc_history = train_ultra_custom(model, config, train_loader, val_loader)
    end_time = time.time()

    # ä½¿ç”¨è®­ç»ƒå†å²è®°å½•é¿å…æœªä½¿ç”¨å˜é‡è­¦å‘Š
    if len(train_loss_history) > 0:
        print(f"   æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_loss_history[-1]:.4f}")

    print("-" * 60)
    print(f"\nè®­ç»ƒå®Œæˆï¼")
    print(f"æ€»è®­ç»ƒæ—¶é—´: {end_time - start_time:.2f}ç§’ ({(end_time - start_time)/60:.1f}åˆ†é’Ÿ)")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {max(val_acc_history):.4f}")

    print("\n5. åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°...")
    print("-" * 60)

    # åŠ è½½æœ€ä½³æ¨¡å‹
    if os.path.exists(config.save_path):
        print(f"åŠ è½½æœ€ä½³æ¨¡å‹: {config.save_path}")
        checkpoint = torch.load(config.save_path, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(device)

        # è¯„ä¼°æµ‹è¯•é›†
        test_acc = evaluate(model, test_loader, config)
        print(f"\næµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.4f}")

        # ä¸åŸå§‹TextCNNæ¯”è¾ƒ
        baseline_acc = 0.8827  # åŸå§‹TextCNNçš„88.27%
        improvement = test_acc - baseline_acc
        print(f"\næ€§èƒ½å¯¹æ¯”:")
        print(f"   åŸå§‹TextCNN: {baseline_acc:.4f}")
        print(f"   æé™ç‰ˆTextCNN: {test_acc:.4f}")
        print(f"   æ€§èƒ½æå‡: {improvement:+.4f} ({improvement/baseline_acc*100:+.2f}%)")

        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°90%ç›®æ ‡
        if test_acc >= 0.90:
            print("\nğŸ‰ æ­å–œï¼æˆåŠŸçªç ´90%å‡†ç¡®ç‡å¤§å…³ï¼")
        else:
            gap = 0.90 - test_acc
            print(f"\nğŸ“ˆ è·ç¦»90%ç›®æ ‡è¿˜å·® {gap:.4f}")

        return test_acc
    else:
        print(f"é”™è¯¯ï¼šæœªæ‰¾åˆ°ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶ {config.save_path}")
        return 0.0

if __name__ == '__main__':
    os.makedirs('save_model', exist_ok=True)

    test_acc = train_ultra_model()

    print(f'\næé™ç‰ˆTextCNNæœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}')
    print('ç›®æ ‡ï¼šçªç ´90%å¤§å…³ï¼')