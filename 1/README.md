# 文本爬取与词向量实践

## 项目简介

本项目完成了从数据采集到词向量训练的完整NLP流程，包括网络爬虫、文本预处理、统计分析和Word2Vec模型训练。

## 作业内容

- **数据采集**: 从古登堡计划爬取英文文学作品，从新浪新闻爬取中文新闻语料
- **并发爬虫**: 实现基于线程池的高效并发爬取策略
- **文本预处理**: 中英文分词、停用词过滤、词形还原
- **统计分析**: 词频分布、词云可视化、齐夫定律验证
- **词向量训练**: 使用Skip-gram算法训练Word2Vec模型
- **对比实验**: 
  - 领域对比（通用新闻 vs 科技新闻）
  - 算法对比（Skip-gram vs CBOW）
  - 超参数对比（负采样数量5 vs 15）

## 主要结果

- 英文语料：138万词，4.3万独立词汇
- 中文语料：80万词，4.6万独立词汇
- 验证了齐夫定律在中英文语料中的适用性
- 通过对比实验揭示了词向量的语义漂移现象

## 项目结构

```
├── data/           # 原始和处理后的语料
├── output/         # 分析结果、图表和模型
├── src/            # Python脚本（按执行顺序命名）
└── report.pdf      # 实验报告
```

## 运行方式

按照 `src/` 目录中脚本的编号顺序依次执行：

1. `1_crawl_english.py` - 爬取英文文本
2. `2_crawl_chinese.py` - 爬取中文新闻
3. `7_crawl_chinese_tech.py` - 爬取科技新闻
4. `3_preprocess_text.py` - 文本预处理
5. `4_analyze_and_visualize.py` - 统计分析
6. `5_train_word2vec.py` - 训练词向量
7. `6_find_similar_words.py` - 相似词查询
8. `8_advanced_analysis.py` - 对比实验
